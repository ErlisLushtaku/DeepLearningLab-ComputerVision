{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2486696",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise Image-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a5d5148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T16:47:37.010939Z",
     "start_time": "2023-04-26T16:47:37.006064Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import paths\n",
    "from datasets import VOCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591da245-9ab5-43cc-8df2-17a5c31cd7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Image Captioning\n",
    "# Your first task will be to complete the inference code to generate captions for the given VOC dataset.\n",
    "from eval_captioning import extract_evaluate_write_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e3ce6b3-fd3d-481a-b164-e7731d3d385a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 16:33:46,692 [INFO] Running on device: cpu, cuda available: False\n",
      "2024-06-05 16:33:50,104 [INFO] Done initializing weights for BertModel BertModel.\n",
      "2024-06-05 16:33:52,063 [INFO] Done initializing weights for BertModel BertLMHeadModel.\n",
      "2024-06-05 16:33:52,122 [INFO] Created model BlipCaption with 247.4M parameters.\n",
      "2024-06-05 16:33:52,705 [INFO] Reshaped position embedding from 196 to 576\n",
      "2024-06-05 16:33:52,789 [INFO] Missing keys []\n",
      "2024-06-05 16:33:52,789 [INFO] Done loading checkpoint from ckpt/blip_model_base.pth\n",
      "2024-06-05 16:33:52,821 [INFO] Load dataset from data/VOCdevkit/VOC2012\n",
      "2024-06-05 16:33:52,861 [INFO] Output dir: outputs/eval_captioning/2024_06_05_16_33_52\n",
      "Captioning images:   0%|                                                                                                                                 | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 16:34:00,238 [INFO] Datapoint 0 got output ['a plane on the ground at an airport', 'two trains parked on the tracks', 'a boat in the water near a dock', 'a train at a train station', 'a group of people riding bikes', 'two sheep in the grass together', 'a computer monitor and a keyboard', 'two people sitting on a train', 'a horse standing in the grass', 'a woman holding a bottle of vodka', 'a cat laying on a couch', 'two cows on the beach with the ocean in the background', 'a cow laying down in a pen', 'a cruise ship in the distance', 'a computer monitor and speakers on a desk', 'a bike parked on the side of the road']\n",
      "Captioning images:   1%|█▎                                                                                                                       | 1/91 [00:07<11:02,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:   2%|██▋                                                                                                                      | 2/91 [00:14<10:25,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:   3%|███▉                                                                                                                     | 3/91 [00:20<10:08,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:   4%|█████▎                                                                                                                   | 4/91 [00:28<10:08,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:   5%|██████▋                                                                                                                  | 5/91 [00:34<09:53,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:   7%|███████▉                                                                                                                 | 6/91 [00:41<09:37,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:   8%|█████████▎                                                                                                               | 7/91 [00:47<09:14,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:   9%|██████████▋                                                                                                              | 8/91 [00:54<09:20,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "decoder_out.shape torch.Size([16, 17, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  10%|███████████▉                                                                                                             | 9/91 [01:02<09:51,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 18, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 19])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 19\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  11%|█████████████▏                                                                                                          | 10/91 [01:09<09:20,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  12%|██████████████▌                                                                                                         | 11/91 [01:15<08:57,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  13%|███████████████▊                                                                                                        | 12/91 [01:22<08:58,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  14%|█████████████████▏                                                                                                      | 13/91 [01:28<08:41,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  15%|██████████████████▍                                                                                                     | 14/91 [01:35<08:25,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  16%|███████████████████▊                                                                                                    | 15/91 [01:41<08:15,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  18%|█████████████████████                                                                                                   | 16/91 [01:47<08:04,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "decoder_out.shape torch.Size([16, 17, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 18])\n",
      "decoder_out.shape torch.Size([16, 18, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 19])\n",
      "decoder_out.shape torch.Size([16, 19, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 20])\n",
      "decoder_out.shape torch.Size([16, 20, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 21])\n",
      "decoder_out.shape torch.Size([16, 21, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  19%|██████████████████████▍                                                                                                 | 17/91 [01:57<09:17,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 22, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 23])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 23\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  20%|███████████████████████▋                                                                                                | 18/91 [02:05<09:00,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  21%|█████████████████████████                                                                                               | 19/91 [02:12<08:45,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  22%|██████████████████████████▎                                                                                             | 20/91 [02:19<08:50,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 17, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 18])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 18\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  23%|███████████████████████████▋                                                                                            | 21/91 [02:27<08:35,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  24%|█████████████████████████████                                                                                           | 22/91 [02:34<08:30,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  25%|██████████████████████████████▎                                                                                         | 23/91 [02:41<08:10,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  26%|███████████████████████████████▋                                                                                        | 24/91 [02:47<07:51,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  27%|████████████████████████████████▉                                                                                       | 25/91 [02:54<07:29,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  29%|██████████████████████████████████▎                                                                                     | 26/91 [03:00<07:13,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  30%|███████████████████████████████████▌                                                                                    | 27/91 [03:07<07:08,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  31%|████████████████████████████████████▉                                                                                   | 28/91 [03:14<07:18,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  32%|██████████████████████████████████████▏                                                                                 | 29/91 [03:21<07:13,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  33%|███████████████████████████████████████▌                                                                                | 30/91 [03:29<07:14,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "decoder_out.shape torch.Size([16, 17, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 18])\n",
      "decoder_out.shape torch.Size([16, 18, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 19])\n",
      "decoder_out.shape torch.Size([16, 19, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 20])\n",
      "decoder_out.shape torch.Size([16, 20, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 21])\n",
      "decoder_out.shape torch.Size([16, 21, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  34%|████████████████████████████████████████▉                                                                               | 31/91 [03:39<07:56,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 22, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 23])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 23\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  35%|██████████████████████████████████████████▏                                                                             | 32/91 [03:46<07:39,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  36%|███████████████████████████████████████████▌                                                                            | 33/91 [03:54<07:32,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 17, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 18])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 18\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  37%|████████████████████████████████████████████▊                                                                           | 34/91 [04:02<07:19,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  38%|██████████████████████████████████████████████▏                                                                         | 35/91 [04:09<07:05,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  40%|███████████████████████████████████████████████▍                                                                        | 36/91 [04:16<06:42,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  41%|████████████████████████████████████████████████▊                                                                       | 37/91 [04:23<06:30,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  42%|██████████████████████████████████████████████████                                                                      | 38/91 [04:28<06:01,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 13\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  43%|███████████████████████████████████████████████████▍                                                                    | 39/91 [04:36<06:05,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  44%|████████████████████████████████████████████████████▋                                                                   | 40/91 [04:43<06:05,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  45%|██████████████████████████████████████████████████████                                                                  | 41/91 [04:51<06:02,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  46%|███████████████████████████████████████████████████████▍                                                                | 42/91 [04:58<05:54,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  47%|████████████████████████████████████████████████████████▋                                                               | 43/91 [05:05<05:43,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  48%|██████████████████████████████████████████████████████████                                                              | 44/91 [05:13<05:41,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  49%|███████████████████████████████████████████████████████████▎                                                            | 45/91 [05:20<05:31,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  51%|████████████████████████████████████████████████████████████▋                                                           | 46/91 [05:26<05:11,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  52%|█████████████████████████████████████████████████████████████▉                                                          | 47/91 [05:33<05:11,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  53%|███████████████████████████████████████████████████████████████▎                                                        | 48/91 [05:40<05:01,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  54%|████████████████████████████████████████████████████████████████▌                                                       | 49/91 [05:47<04:54,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "decoder_out.shape torch.Size([16, 17, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 18])\n",
      "decoder_out.shape torch.Size([16, 18, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 19])\n",
      "decoder_out.shape torch.Size([16, 19, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 20])\n",
      "decoder_out.shape torch.Size([16, 20, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 21])\n",
      "decoder_out.shape torch.Size([16, 21, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  55%|█████████████████████████████████████████████████████████████████▉                                                      | 50/91 [05:57<05:23,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 22, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 23])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 23\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  56%|███████████████████████████████████████████████████████████████████▎                                                    | 51/91 [06:04<05:06,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  57%|████████████████████████████████████████████████████████████████████▌                                                   | 52/91 [06:12<04:56,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  58%|█████████████████████████████████████████████████████████████████████▉                                                  | 53/91 [06:18<04:34,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  59%|███████████████████████████████████████████████████████████████████████▏                                                | 54/91 [06:25<04:20,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  60%|████████████████████████████████████████████████████████████████████████▌                                               | 55/91 [06:31<04:09,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  62%|█████████████████████████████████████████████████████████████████████████▊                                              | 56/91 [06:38<03:56,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  63%|███████████████████████████████████████████████████████████████████████████▏                                            | 57/91 [06:44<03:40,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 13\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  64%|████████████████████████████████████████████████████████████████████████████▍                                           | 58/91 [06:51<03:48,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 17, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 18])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 18\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  65%|█████████████████████████████████████████████████████████████████████████████▊                                          | 59/91 [06:58<03:41,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  66%|███████████████████████████████████████████████████████████████████████████████                                         | 60/91 [07:06<03:36,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  67%|████████████████████████████████████████████████████████████████████████████████▍                                       | 61/91 [07:12<03:26,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  68%|█████████████████████████████████████████████████████████████████████████████████▊                                      | 62/91 [07:19<03:21,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  69%|███████████████████████████████████████████████████████████████████████████████████                                     | 63/91 [07:27<03:22,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 17, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 18])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 18\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  70%|████████████████████████████████████████████████████████████████████████████████████▍                                   | 64/91 [07:34<03:11,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 65/91 [07:41<03:00,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  73%|███████████████████████████████████████████████████████████████████████████████████████                                 | 66/91 [07:48<02:54,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  74%|████████████████████████████████████████████████████████████████████████████████████████▎                               | 67/91 [07:54<02:42,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  75%|█████████████████████████████████████████████████████████████████████████████████████████▋                              | 68/91 [08:00<02:33,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  76%|██████████████████████████████████████████████████████████████████████████████████████████▉                             | 69/91 [08:07<02:26,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  77%|████████████████████████████████████████████████████████████████████████████████████████████▎                           | 70/91 [08:14<02:20,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  78%|█████████████████████████████████████████████████████████████████████████████████████████████▋                          | 71/91 [08:20<02:13,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  79%|██████████████████████████████████████████████████████████████████████████████████████████████▉                         | 72/91 [08:26<02:02,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 13\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  80%|████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 73/91 [08:34<02:02,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  81%|█████████████████████████████████████████████████████████████████████████████████████████████████▌                      | 74/91 [08:41<01:58,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  82%|██████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 75/91 [08:48<01:50,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                   | 76/91 [08:55<01:43,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 77/91 [09:01<01:35,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 78/91 [09:08<01:26,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 79/91 [09:15<01:20,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 80/91 [09:22<01:15,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 81/91 [09:29<01:08,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 82/91 [09:35<01:01,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍          | 83/91 [09:42<00:54,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 84/91 [09:50<00:49,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 17\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████        | 85/91 [09:57<00:42,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 86/91 [10:03<00:34,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 87/91 [10:10<00:27,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 88/91 [10:17<00:20,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 15\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 89/91 [10:24<00:13,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 16\n",
      "initial input_ids.shape torch.Size([16, 4])\n",
      "decoder_out.shape torch.Size([16, 4, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 5])\n",
      "decoder_out.shape torch.Size([16, 5, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 6])\n",
      "decoder_out.shape torch.Size([16, 6, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 7])\n",
      "decoder_out.shape torch.Size([16, 7, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 8])\n",
      "decoder_out.shape torch.Size([16, 8, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 9])\n",
      "decoder_out.shape torch.Size([16, 9, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 10])\n",
      "decoder_out.shape torch.Size([16, 10, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 11])\n",
      "decoder_out.shape torch.Size([16, 11, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 12])\n",
      "decoder_out.shape torch.Size([16, 12, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 13])\n",
      "decoder_out.shape torch.Size([16, 13, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 14])\n",
      "decoder_out.shape torch.Size([16, 14, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 15])\n",
      "decoder_out.shape torch.Size([16, 15, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 16])\n",
      "decoder_out.shape torch.Size([16, 16, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 90/91 [10:31<00:07,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([16, 17, 30524])\n",
      "next_token_logits.shape torch.Size([16, 30524])\n",
      "next_tokens_logits.shape torch.Size([16, 30524])\n",
      "next_tokens.shape torch.Size([16])\n",
      "next_tokens.shape torch.Size([16, 1])\n",
      "input_ids.shape torch.Size([16, 18])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 18\n",
      "initial input_ids.shape torch.Size([9, 4])\n",
      "decoder_out.shape torch.Size([9, 4, 30524])\n",
      "next_token_logits.shape torch.Size([9, 30524])\n",
      "next_tokens_logits.shape torch.Size([9, 30524])\n",
      "next_tokens.shape torch.Size([9])\n",
      "next_tokens.shape torch.Size([9, 1])\n",
      "input_ids.shape torch.Size([9, 5])\n",
      "decoder_out.shape torch.Size([9, 5, 30524])\n",
      "next_token_logits.shape torch.Size([9, 30524])\n",
      "next_tokens_logits.shape torch.Size([9, 30524])\n",
      "next_tokens.shape torch.Size([9])\n",
      "next_tokens.shape torch.Size([9, 1])\n",
      "input_ids.shape torch.Size([9, 6])\n",
      "decoder_out.shape torch.Size([9, 6, 30524])\n",
      "next_token_logits.shape torch.Size([9, 30524])\n",
      "next_tokens_logits.shape torch.Size([9, 30524])\n",
      "next_tokens.shape torch.Size([9])\n",
      "next_tokens.shape torch.Size([9, 1])\n",
      "input_ids.shape torch.Size([9, 7])\n",
      "decoder_out.shape torch.Size([9, 7, 30524])\n",
      "next_token_logits.shape torch.Size([9, 30524])\n",
      "next_tokens_logits.shape torch.Size([9, 30524])\n",
      "next_tokens.shape torch.Size([9])\n",
      "next_tokens.shape torch.Size([9, 1])\n",
      "input_ids.shape torch.Size([9, 8])\n",
      "decoder_out.shape torch.Size([9, 8, 30524])\n",
      "next_token_logits.shape torch.Size([9, 30524])\n",
      "next_tokens_logits.shape torch.Size([9, 30524])\n",
      "next_tokens.shape torch.Size([9])\n",
      "next_tokens.shape torch.Size([9, 1])\n",
      "input_ids.shape torch.Size([9, 9])\n",
      "decoder_out.shape torch.Size([9, 9, 30524])\n",
      "next_token_logits.shape torch.Size([9, 30524])\n",
      "next_tokens_logits.shape torch.Size([9, 30524])\n",
      "next_tokens.shape torch.Size([9])\n",
      "next_tokens.shape torch.Size([9, 1])\n",
      "input_ids.shape torch.Size([9, 10])\n",
      "decoder_out.shape torch.Size([9, 10, 30524])\n",
      "next_token_logits.shape torch.Size([9, 30524])\n",
      "next_tokens_logits.shape torch.Size([9, 30524])\n",
      "next_tokens.shape torch.Size([9])\n",
      "next_tokens.shape torch.Size([9, 1])\n",
      "input_ids.shape torch.Size([9, 11])\n",
      "decoder_out.shape torch.Size([9, 11, 30524])\n",
      "next_token_logits.shape torch.Size([9, 30524])\n",
      "next_tokens_logits.shape torch.Size([9, 30524])\n",
      "next_tokens.shape torch.Size([9])\n",
      "next_tokens.shape torch.Size([9, 1])\n",
      "input_ids.shape torch.Size([9, 12])\n",
      "decoder_out.shape torch.Size([9, 12, 30524])\n",
      "next_token_logits.shape torch.Size([9, 30524])\n",
      "next_tokens_logits.shape torch.Size([9, 30524])\n",
      "next_tokens.shape torch.Size([9])\n",
      "next_tokens.shape torch.Size([9, 1])\n",
      "input_ids.shape torch.Size([9, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [10:35<00:00,  6.99s/it]\n",
      "2024-06-05 16:44:28,761 [INFO] Writing 1449 captions to outputs/eval_captioning/2024_06_05_16_33_52/pred_captions.txt\n",
      "2024-06-05 16:44:28,762 [INFO] Writing 1449 captions to outputs/eval_captioning/2024_06_05_16_33_52/ref_captions.txt\n",
      "2024-06-05 16:44:28,763 [INFO] Evaluating captions\n",
      "2024-06-05 16:44:28,825 [INFO] BLEU 1-grams: 0.5678858282540832, 2-grams: 0.4415108475968761, 3-grams: 0.3473413509347541, 4-grams: 0.27953393436609436\n",
      "2024-06-05 16:44:28,826 [INFO] Final BLEU@4: 27.95%\n",
      "2024-06-05 16:44:28,826 [INFO] Scores: {'bleu': 0.27953393436609436}\n",
      "2024-06-05 16:44:28,826 [INFO] Writing scores to outputs/eval_captioning/2024_06_05_16_33_52/scores.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_out.shape torch.Size([9, 13, 30524])\n",
      "next_token_logits.shape torch.Size([9, 30524])\n",
      "next_tokens_logits.shape torch.Size([9, 30524])\n",
      "next_tokens.shape torch.Size([9])\n",
      "next_tokens.shape torch.Size([9, 1])\n",
      "input_ids.shape torch.Size([9, 14])\n",
      "unfinished_sequences.max() tensor(0)\n",
      "input_ids.shape[-1] 14\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Complete Caption Generation with Greedy Search\n",
    "\n",
    "# TODO: In file models/blip/blip_caption.py complete the methods generate and greedy_search. \n",
    "#       Generate and evaluate captions for the VOC dataset. You should get about 28% BLEU score. (2 points)\n",
    "\n",
    "extract_evaluate_write_captions(use_topk_sampling=False, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1455059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T16:47:37.256254Z",
     "start_time": "2023-04-26T16:47:37.192994Z"
    }
   },
   "outputs": [],
   "source": [
    "voc_path = Path(paths.CV_PATH_VOC)\n",
    "dataset = VOCDataset(voc_path, voc_path / \"ImageSets\" / \"Segmentation\" / \"val.txt\",\n",
    "                     load_captions=True)\n",
    "\n",
    "# load and show generated captions\n",
    "# todo update the path to match your experiment\n",
    "pred_captions_file = \"outputs/eval_captioning/2024_06_05_16_33_52/pred_captions.txt\"\n",
    "with open(pred_captions_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    pred_captions = f.readlines()\n",
    "    \n",
    "from PIL import Image\n",
    "for i in range(10):\n",
    "    data = dataset[i]\n",
    "    display(data[\"image\"])\n",
    "    print(f\"Pred caption: {pred_captions[i]}\")\n",
    "    print(f\"Reference caption: {data['caption']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eceaf88",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 19:23:35,043 [INFO] Running on device: cpu, cuda available: False\n",
      "2024-06-05 19:23:38,454 [INFO] Done initializing weights for BertModel BertModel.\n",
      "2024-06-05 19:23:40,373 [INFO] Done initializing weights for BertModel BertLMHeadModel.\n",
      "2024-06-05 19:23:40,457 [INFO] Created model BlipCaption with 247.4M parameters.\n",
      "2024-06-05 19:23:40,744 [INFO] Reshaped position embedding from 196 to 576\n",
      "2024-06-05 19:23:40,792 [INFO] Missing keys []\n",
      "2024-06-05 19:23:40,792 [INFO] Done loading checkpoint from ckpt/blip_model_base.pth\n",
      "2024-06-05 19:23:40,826 [INFO] Load dataset from data/VOCdevkit/VOC2012\n",
      "2024-06-05 19:23:40,866 [INFO] Output dir: outputs/eval_captioning/2024_06_05_19_23_40\n",
      "2024-06-05 19:23:49,185 [INFO] Datapoint 0 got output ['a white airplanes landing gate before landing … others, including a', 'two trains that are sitting next to each other piers on a track', 'a boat tied up to a pier others boats are sitting on the', 'a old train on some trackspwa [ ] with', 'a group of bikers riding down the street9115', 'two sheep lying on top of each other in the grass celestial', 'a computer monitor sitting on a desk and a tv at the', 'two people on a subway looking at the camerashore guaranteen', 'a horse standing under a tent his ladder out side — i', 'me with my vodka bottle in my lap 歌 from @', 'a cat laying on the back of a chair with the door open', 'a cow on a beach with the sea in the background suburban port', 'dairy cows eating hay and hay bales null youtubeeke', 'a cruise ship on the watershore from drivingfarellore', 'a keyboard, monitor, headphones and keyboard regard to a room', \"bicycle parked for someone's bicycle ride at nightbardes\"]\n",
      "Captioning images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [12:43<00:00,  8.39s/it]\n",
      "2024-06-05 19:36:24,690 [INFO] Writing 1449 captions to outputs/eval_captioning/2024_06_05_19_23_40/pred_captions.txt\n",
      "2024-06-05 19:36:24,692 [INFO] Writing 1449 captions to outputs/eval_captioning/2024_06_05_19_23_40/ref_captions.txt\n",
      "2024-06-05 19:36:24,693 [INFO] Evaluating captions\n",
      "2024-06-05 19:36:24,767 [INFO] BLEU 1-grams: 0.28879487631469014, 2-grams: 0.17366436244309114, 3-grams: 0.10557696812765222, 4-grams: 0.06556602691354341\n",
      "2024-06-05 19:36:24,768 [INFO] Final BLEU@4: 6.56%\n",
      "2024-06-05 19:36:24,768 [INFO] Scores: {'bleu': 0.06556602691354341}\n",
      "2024-06-05 19:36:24,768 [INFO] Writing scores to outputs/eval_captioning/2024_06_05_19_23_40/scores.json\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Complete Caption Generation with Sampling\n",
    "# TODO: In file models/blip/blip_caption.py complete the method sampling. \n",
    "#       There, Top-K sampling instead of greedy search is used to select the next token when decoding. \n",
    "#       Evaluate again with Top-K sampling.\n",
    "#       You should get a lower BLEU score of about 7% for temperature τ = 1.0, and about 12% for τ = 0.7. \n",
    "#       Why do the results improve with lower temperature? (1 point)\n",
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=50, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf0f95e-95e1-4eaf-b6c4-387268bfc237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 19:37:55,323 [INFO] Running on device: cpu, cuda available: False\n",
      "2024-06-05 19:37:58,834 [INFO] Done initializing weights for BertModel BertModel.\n",
      "2024-06-05 19:38:00,799 [INFO] Done initializing weights for BertModel BertLMHeadModel.\n",
      "2024-06-05 19:38:00,859 [INFO] Created model BlipCaption with 247.4M parameters.\n",
      "2024-06-05 19:38:01,188 [INFO] Reshaped position embedding from 196 to 576\n",
      "2024-06-05 19:38:01,237 [INFO] Missing keys []\n",
      "2024-06-05 19:38:01,238 [INFO] Done loading checkpoint from ckpt/blip_model_base.pth\n",
      "2024-06-05 19:38:01,270 [INFO] Load dataset from data/VOCdevkit/VOC2012\n",
      "2024-06-05 19:38:01,302 [INFO] Output dir: outputs/eval_captioning/2024_06_05_19_38_01\n",
      "2024-06-05 19:38:11,222 [INFO] Datapoint 0 got output ['a plane from the air port where it is being towed to the airport', 'two trains that are sitting next to each other piers in front of a', 'a boat in the water next to trees gazetteshore letters of the', 'a train on a track with a train and a person walking on one side', 'a group of cyclists riding on a road an indoor track phones are on', 'two sheep in a field of grass null and two small sheep resting in', 'a computer monitor sitting on a desk and a tv at the end', 'two people on a train talking on their cell phones indonesia nully', 'a horse standing near a slide surfing staten islandicialcoming', 'a woman holding a bottle of wine others are also in the background –', 'a cat laying on the back of a chair itself promise to be a', 'a cow on a beach with the ocean in the background jpg snaps', 'a cow that is laying down facility in a barn georgia', 'a cruise ship on the watershore from a car socio - economic problems', 'a computer with a cat sitting in front of it mean threshold an', \"a bike out at night with a street sign till it's gone\"]\n",
      "Captioning images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [13:48<00:00,  9.10s/it]\n",
      "2024-06-05 19:51:49,488 [INFO] Writing 1449 captions to outputs/eval_captioning/2024_06_05_19_38_01/pred_captions.txt\n",
      "2024-06-05 19:51:49,490 [INFO] Writing 1449 captions to outputs/eval_captioning/2024_06_05_19_38_01/ref_captions.txt\n",
      "2024-06-05 19:51:49,491 [INFO] Evaluating captions\n",
      "2024-06-05 19:51:49,561 [INFO] BLEU 1-grams: 0.3949711179068709, 2-grams: 0.2687005433159694, 3-grams: 0.18480344310941946, 4-grams: 0.12900651574910763\n",
      "2024-06-05 19:51:49,561 [INFO] Final BLEU@4: 12.90%\n",
      "2024-06-05 19:51:49,562 [INFO] Scores: {'bleu': 0.12900651574910763}\n",
      "2024-06-05 19:51:49,562 [INFO] Writing scores to outputs/eval_captioning/2024_06_05_19_38_01/scores.json\n"
     ]
    }
   ],
   "source": [
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=50, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b4abaff-6d80-43dc-9e2f-4377b09548b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 20:13:12,552 [INFO] Running on device: cpu, cuda available: False\n",
      "2024-06-05 20:13:16,313 [INFO] Done initializing weights for BertModel BertModel.\n",
      "2024-06-05 20:13:18,512 [INFO] Done initializing weights for BertModel BertLMHeadModel.\n",
      "2024-06-05 20:13:18,542 [INFO] Created model BlipCaption with 247.4M parameters.\n",
      "2024-06-05 20:13:18,771 [INFO] Reshaped position embedding from 196 to 576\n",
      "2024-06-05 20:13:18,822 [INFO] Missing keys []\n",
      "2024-06-05 20:13:18,823 [INFO] Done loading checkpoint from ckpt/blip_model_base.pth\n",
      "2024-06-05 20:13:18,853 [INFO] Load dataset from data/VOCdevkit/VOC2012\n",
      "2024-06-05 20:13:18,873 [INFO] Output dir: outputs/eval_captioning/2024_06_05_20_13_18\n",
      "\n",
      "Captioning images:   0%|                                                                                                                                 | 0/91 [00:00<?, ?it/s]\u001b[A\n",
      "2024-06-05 20:13:29,436 [INFO] Datapoint 0 got output ['a passenger plane being serviced at the airport jpeg 歌', 'two trains on a railroad track malaga and the text how', 'boats moored in a canal at the end of a city', 'a train coming into a station mute others are coming', 'a group of cyclists riding on a road at night lesbian', 'two sheep in a meadow of grass [ photo by flickr', 'a computer and other electronics in a room [ ] script', 'two people in a subway train – and one is talking on', 'a horse and a slide in the woods itself null', 'a woman holding a bottle of vodka surfing swatch', 'a living room with two couches and a tvshore', 'a beach and seagulls and water backwards its', 'a cow in a barn eating hay facility on a farm', 'a cruise ship docked behind a palm tree in fort lauderdale possible', 'a computer and a desk with headphones surfing mean', 'city lights by the road and a bicycle parked on the sidewalk']\n",
      "\n",
      "Captioning images:  11%|█████████████▏                                                                                                          | 10/91 [03:26<22:25, 16.61s/it]\u001b[A\n",
      "Captioning images:   1%|█▎                                                                                                                       | 1/91 [00:10<15:51, 10.57s/it]\u001b[A\n",
      "Captioning images:   2%|██▋                                                                                                                      | 2/91 [00:21<15:52, 10.70s/it]\u001b[A\n",
      "Captioning images:   3%|███▉                                                                                                                     | 3/91 [00:37<19:18, 13.16s/it]\u001b[A\n",
      "Captioning images:   4%|█████▎                                                                                                                   | 4/91 [00:48<17:47, 12.27s/it]\u001b[A\n",
      "Captioning images:   5%|██████▋                                                                                                                  | 5/91 [01:00<17:33, 12.25s/it]\u001b[A\n",
      "Captioning images:   7%|███████▉                                                                                                                 | 6/91 [01:12<17:12, 12.15s/it]\u001b[A\n",
      "Captioning images:   8%|█████████▎                                                                                                               | 7/91 [01:24<16:59, 12.14s/it]\u001b[A\n",
      "Captioning images:   9%|██████████▋                                                                                                              | 8/91 [01:34<15:58, 11.55s/it]\u001b[A\n",
      "Captioning images:  10%|███████████▉                                                                                                             | 9/91 [01:48<16:34, 12.13s/it]\u001b[A\n",
      "Captioning images:  11%|█████████████▏                                                                                                          | 10/91 [01:59<16:02, 11.88s/it]\u001b[A\n",
      "Captioning images:  12%|██████████████▌                                                                                                         | 11/91 [02:11<15:38, 11.73s/it]\u001b[A\n",
      "Captioning images:  13%|███████████████▊                                                                                                        | 12/91 [02:19<14:19, 10.88s/it]\u001b[A\n",
      "Captioning images:  14%|█████████████████▏                                                                                                      | 13/91 [02:30<14:06, 10.85s/it]\u001b[A\n",
      "Captioning images:  15%|██████████████████▍                                                                                                     | 14/91 [02:39<13:05, 10.21s/it]\u001b[A\n",
      "Captioning images:  16%|███████████████████▊                                                                                                    | 15/91 [02:48<12:18,  9.72s/it]\u001b[A\n",
      "Captioning images:  18%|█████████████████████                                                                                                   | 16/91 [03:02<13:50, 11.07s/it]\u001b[A\n",
      "Captioning images:  19%|██████████████████████▍                                                                                                 | 17/91 [03:13<13:44, 11.15s/it]\u001b[A\n",
      "Captioning images:  20%|███████████████████████▋                                                                                                | 18/91 [03:23<12:58, 10.67s/it]\u001b[A\n",
      "Captioning images:  21%|█████████████████████████                                                                                               | 19/91 [03:34<13:12, 11.00s/it]\u001b[A\n",
      "Captioning images:  22%|██████████████████████████▎                                                                                             | 20/91 [03:47<13:44, 11.61s/it]\u001b[A\n",
      "Captioning images:  23%|███████████████████████████▋                                                                                            | 21/91 [03:58<12:59, 11.14s/it]\u001b[A\n",
      "Captioning images:  24%|█████████████████████████████                                                                                           | 22/91 [04:08<12:27, 10.83s/it]\u001b[A\n",
      "Captioning images:  25%|██████████████████████████████▎                                                                                         | 23/91 [04:20<12:40, 11.19s/it]\u001b[A\n",
      "Captioning images:  26%|███████████████████████████████▋                                                                                        | 24/91 [04:32<12:50, 11.50s/it]\u001b[A\n",
      "Captioning images:  27%|████████████████████████████████▉                                                                                       | 25/91 [04:44<12:43, 11.56s/it]\u001b[A\n",
      "Captioning images:  29%|██████████████████████████████████▎                                                                                     | 26/91 [04:54<12:07, 11.19s/it]\u001b[A\n",
      "Captioning images:  30%|███████████████████████████████████▌                                                                                    | 27/91 [05:05<11:59, 11.25s/it]\u001b[A\n",
      "Captioning images:  31%|████████████████████████████████████▉                                                                                   | 28/91 [05:16<11:39, 11.11s/it]\u001b[A\n",
      "Captioning images:  32%|██████████████████████████████████████▏                                                                                 | 29/91 [05:36<14:10, 13.72s/it]\u001b[A\n",
      "Captioning images:  33%|███████████████████████████████████████▌                                                                                | 30/91 [05:46<12:50, 12.63s/it]\u001b[A\n",
      "Captioning images:  34%|████████████████████████████████████████▉                                                                               | 31/91 [05:58<12:35, 12.59s/it]\u001b[A\n",
      "Captioning images:  35%|██████████████████████████████████████████▏                                                                             | 32/91 [06:11<12:22, 12.58s/it]\u001b[A\n",
      "Captioning images:  36%|███████████████████████████████████████████▌                                                                            | 33/91 [06:23<11:52, 12.29s/it]\u001b[A\n",
      "Captioning images:  37%|████████████████████████████████████████████▊                                                                           | 34/91 [06:34<11:18, 11.91s/it]\u001b[A\n",
      "Captioning images:  38%|██████████████████████████████████████████████▏                                                                         | 35/91 [06:43<10:26, 11.18s/it]\u001b[A\n",
      "Captioning images:  40%|███████████████████████████████████████████████▍                                                                        | 36/91 [06:55<10:19, 11.26s/it]\u001b[A\n",
      "Captioning images:  41%|████████████████████████████████████████████████▊                                                                       | 37/91 [07:06<10:08, 11.28s/it]\u001b[A\n",
      "Captioning images:  42%|██████████████████████████████████████████████████                                                                      | 38/91 [07:17<09:58, 11.30s/it]\u001b[A\n",
      "Captioning images:  43%|███████████████████████████████████████████████████▍                                                                    | 39/91 [07:29<09:58, 11.51s/it]\u001b[A\n",
      "Captioning images:  44%|████████████████████████████████████████████████████▋                                                                   | 40/91 [07:40<09:35, 11.28s/it]\u001b[A\n",
      "Captioning images:  45%|██████████████████████████████████████████████████████                                                                  | 41/91 [07:51<09:26, 11.33s/it]\u001b[A\n",
      "Captioning images:  46%|███████████████████████████████████████████████████████▍                                                                | 42/91 [08:02<08:58, 10.99s/it]\u001b[A\n",
      "Captioning images:  47%|████████████████████████████████████████████████████████▋                                                               | 43/91 [08:14<09:03, 11.32s/it]\u001b[A\n",
      "Captioning images:  48%|██████████████████████████████████████████████████████████                                                              | 44/91 [08:22<08:11, 10.46s/it]\u001b[A\n",
      "Captioning images:  49%|███████████████████████████████████████████████████████████▎                                                            | 45/91 [08:33<08:04, 10.53s/it]\u001b[A\n",
      "Captioning images:  51%|████████████████████████████████████████████████████████████▋                                                           | 46/91 [08:43<07:54, 10.55s/it]\u001b[A\n",
      "Captioning images:  52%|█████████████████████████████████████████████████████████████▉                                                          | 47/91 [08:54<07:45, 10.59s/it]\u001b[A\n",
      "Captioning images:  53%|███████████████████████████████████████████████████████████████▎                                                        | 48/91 [09:07<07:58, 11.13s/it]\u001b[A\n",
      "Captioning images:  54%|████████████████████████████████████████████████████████████████▌                                                       | 49/91 [09:16<07:28, 10.67s/it]\u001b[A\n",
      "Captioning images:  55%|█████████████████████████████████████████████████████████████████▉                                                      | 50/91 [09:30<07:54, 11.57s/it]\u001b[A\n",
      "Captioning images:  56%|███████████████████████████████████████████████████████████████████▎                                                    | 51/91 [09:41<07:40, 11.52s/it]\u001b[A\n",
      "Captioning images:  57%|████████████████████████████████████████████████████████████████████▌                                                   | 52/91 [09:52<07:20, 11.29s/it]\u001b[A\n",
      "Captioning images:  58%|████████████████████████████████████████████████████████████████████▏                                                | 53/91 [15:59<1:14:41, 117.93s/it]\u001b[A\n",
      "Captioning images:  59%|███████████████████████████████████████████████████████████████████████▏                                                | 54/91 [16:10<52:56, 85.85s/it]\u001b[A\n",
      "Captioning images:  60%|████████████████████████████████████████████████████████████████████████▌                                               | 55/91 [16:21<38:06, 63.52s/it]\u001b[A\n",
      "Captioning images:  62%|█████████████████████████████████████████████████████████████████████████▊                                              | 56/91 [16:35<28:17, 48.49s/it]\u001b[A\n",
      "Captioning images:  63%|███████████████████████████████████████████████████████████████████████████▏                                            | 57/91 [16:44<20:47, 36.68s/it]\u001b[A\n",
      "Captioning images:  64%|████████████████████████████████████████████████████████████████████████████▍                                           | 58/91 [16:55<16:00, 29.11s/it]\u001b[A\n",
      "Captioning images:  65%|█████████████████████████████████████████████████████████████████████████████▊                                          | 59/91 [17:05<12:26, 23.31s/it]\u001b[A\n",
      "Captioning images:  66%|███████████████████████████████████████████████████████████████████████████████                                         | 60/91 [17:21<10:53, 21.08s/it]\u001b[A\n",
      "Captioning images:  67%|████████████████████████████████████████████████████████████████████████████████▍                                       | 61/91 [17:32<09:01, 18.06s/it]\u001b[A\n",
      "Captioning images:  68%|█████████████████████████████████████████████████████████████████████████████████▊                                      | 62/91 [17:44<07:56, 16.44s/it]\u001b[A\n",
      "Captioning images:  69%|███████████████████████████████████████████████████████████████████████████████████                                     | 63/91 [17:55<06:48, 14.57s/it]\u001b[A\n",
      "Captioning images:  70%|████████████████████████████████████████████████████████████████████████████████████▍                                   | 64/91 [18:06<06:10, 13.71s/it]\u001b[A\n",
      "Captioning images:  71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 65/91 [18:17<05:30, 12.73s/it]\u001b[A\n",
      "Captioning images:  73%|███████████████████████████████████████████████████████████████████████████████████████                                 | 66/91 [18:27<04:56, 11.84s/it]\u001b[A\n",
      "Captioning images:  74%|████████████████████████████████████████████████████████████████████████████████████████▎                               | 67/91 [18:39<04:48, 12.02s/it]\u001b[A\n",
      "Captioning images:  75%|█████████████████████████████████████████████████████████████████████████████████████████▋                              | 68/91 [18:48<04:15, 11.10s/it]\u001b[A\n",
      "Captioning images:  76%|██████████████████████████████████████████████████████████████████████████████████████████▉                             | 69/91 [18:59<04:00, 10.93s/it]\u001b[A\n",
      "Captioning images:  77%|████████████████████████████████████████████████████████████████████████████████████████████▎                           | 70/91 [19:08<03:40, 10.50s/it]\u001b[A\n",
      "Captioning images:  78%|█████████████████████████████████████████████████████████████████████████████████████████████▋                          | 71/91 [19:19<03:35, 10.77s/it]\u001b[A\n",
      "Captioning images:  79%|██████████████████████████████████████████████████████████████████████████████████████████████▉                         | 72/91 [19:29<03:15, 10.29s/it]\u001b[A\n",
      "Captioning images:  80%|████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 73/91 [19:42<03:21, 11.22s/it]\u001b[A\n",
      "Captioning images:  81%|█████████████████████████████████████████████████████████████████████████████████████████████████▌                      | 74/91 [19:52<03:07, 11.01s/it]\u001b[A\n",
      "Captioning images:  82%|██████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 75/91 [20:05<03:01, 11.36s/it]\u001b[A\n",
      "Captioning images:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                   | 76/91 [20:14<02:41, 10.78s/it]\u001b[A\n",
      "Captioning images:  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 77/91 [20:21<02:13,  9.56s/it]\u001b[A\n",
      "Captioning images:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 78/91 [20:28<01:56,  8.93s/it]\u001b[A\n",
      "Captioning images:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 79/91 [20:37<01:47,  8.98s/it]\u001b[A\n",
      "Captioning images:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 80/91 [20:44<01:29,  8.18s/it]\u001b[A\n",
      "Captioning images:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 81/91 [20:51<01:20,  8.02s/it]\u001b[A\n",
      "Captioning images:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 82/91 [20:58<01:09,  7.74s/it]\u001b[A\n",
      "Captioning images:  91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍          | 83/91 [21:08<01:06,  8.32s/it]\u001b[A\n",
      "Captioning images:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 84/91 [21:15<00:55,  7.99s/it]\u001b[A\n",
      "Captioning images:  93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████        | 85/91 [21:24<00:49,  8.25s/it]\u001b[A\n",
      "Captioning images:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 86/91 [21:34<00:44,  8.83s/it]\u001b[A\n",
      "Captioning images:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 87/91 [21:43<00:34,  8.72s/it]\u001b[A\n",
      "Captioning images:  97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 88/91 [21:50<00:24,  8.27s/it]\u001b[A\n",
      "Captioning images:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 89/91 [21:57<00:15,  7.82s/it]\u001b[A\n",
      "Captioning images:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 90/91 [22:04<00:07,  7.62s/it]\u001b[A\n",
      "Captioning images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [22:08<00:00, 14.60s/it]\u001b[A\n",
      "2024-06-05 20:35:27,639 [INFO] Writing 1449 captions to outputs/eval_captioning/2024_06_05_20_13_18/pred_captions.txt\n",
      "2024-06-05 20:35:27,640 [INFO] Writing 1449 captions to outputs/eval_captioning/2024_06_05_20_13_18/ref_captions.txt\n",
      "2024-06-05 20:35:27,641 [INFO] Evaluating captions\n",
      "2024-06-05 20:35:27,726 [INFO] BLEU 1-grams: 0.3360447381799481, 2-grams: 0.2173250967577811, 3-grams: 0.14069308647952228, 4-grams: 0.09383135440590419\n",
      "2024-06-05 20:35:27,726 [INFO] Final BLEU@4: 9.38%\n",
      "2024-06-05 20:35:27,727 [INFO] Scores: {'bleu': 0.09383135440590419}\n",
      "2024-06-05 20:35:27,727 [INFO] Writing scores to outputs/eval_captioning/2024_06_05_20_13_18/scores.json\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Prompt Engineering\n",
    "# TODO: Experiment with different prompts (the default prompt is “a picture of ”). \n",
    "#       Can you improve the BLEU score? Try at least 3 new settings.\n",
    "#       Note the prompt and the resulting BLEU score in a table for each setting. \n",
    "#       Add the table to your report. (1 point)\n",
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=50, temperature=0.7, prompt=\"a scene showing \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a352c828-0517-482d-a607-278accb7c559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 20:43:24,247 [INFO] Running on device: cpu, cuda available: False\n",
      "2024-06-05 20:43:27,649 [INFO] Done initializing weights for BertModel BertModel.\n",
      "2024-06-05 20:43:29,555 [INFO] Done initializing weights for BertModel BertLMHeadModel.\n",
      "2024-06-05 20:43:29,610 [INFO] Created model BlipCaption with 247.4M parameters.\n",
      "2024-06-05 20:43:29,780 [INFO] Reshaped position embedding from 196 to 576\n",
      "2024-06-05 20:43:29,814 [INFO] Missing keys []\n",
      "2024-06-05 20:43:29,815 [INFO] Done loading checkpoint from ckpt/blip_model_base.pth\n",
      "2024-06-05 20:43:29,840 [INFO] Load dataset from data/VOCdevkit/VOC2012\n",
      "2024-06-05 20:43:29,875 [INFO] Output dir: outputs/eval_captioning/2024_06_05_20_43_29\n",
      "\n",
      "                                                                                                                                                                                \u001b[A\n",
      "\u001b[A                                                                                                                                                                             2024-06-05 20:43:37,914 [INFO] Datapoint 0 got output ['a plane sitting at an airport fictional airporthop', 'two trains on a train track, with a metal building in', 'a boat on the water near some housesliders snaps', 'a train going down some tracks fixture deserted with graffiti', 'a group of people riding on bikeskesstenders', 'two sheep laying in the grass letters i & m left on', 'a computer set up on a desk youtube transition from a', 'two people in front of a mirror odd robotic phones', 'a horse that is in a field at a playground jp', 'a woman holding a bottle of vodkascreen shot of her in', 'a living room with a cat on a couch on a couch', 'two animals on the beach togetherinas possible an ocean', 'a cow on a bench in a barn either of the cows', \"a cruise ship in the distance from a driver's seat\", 'a computer desk with a monitor and speakers surfing meander', 'a bike and a motorcycle parked at a bus stop waterway']\n",
      "Captioning images:  11%|█████████████▏                                                                                                          | 10/91 [33:34<22:25, 16.61s/it]\n",
      "Captioning images:   0%|                                                                                                                                 | 0/91 [00:08<?, ?it/s]\u001b[A\n",
      "Captioning images:   1%|█▎                                                                                                                       | 1/91 [00:08<12:03,  8.04s/it]\u001b[A\n",
      "Captioning images:   2%|██▋                                                                                                                      | 2/91 [00:16<12:12,  8.23s/it]\u001b[A\n",
      "Captioning images:   3%|███▉                                                                                                                     | 3/91 [00:29<15:39, 10.67s/it]\u001b[A\n",
      "Captioning images:   4%|█████▎                                                                                                                   | 4/91 [00:43<16:54, 11.66s/it]\u001b[A\n",
      "Captioning images:   5%|██████▋                                                                                                                  | 5/91 [00:59<19:18, 13.48s/it]\u001b[A\n",
      "Captioning images:   7%|███████▉                                                                                                                 | 6/91 [01:13<19:14, 13.58s/it]\u001b[A\n",
      "Captioning images:   8%|█████████▎                                                                                                               | 7/91 [01:28<19:45, 14.11s/it]\u001b[A\n",
      "Captioning images:   9%|██████████▋                                                                                                              | 8/91 [01:42<19:25, 14.05s/it]\u001b[A\n",
      "Captioning images:  10%|███████████▉                                                                                                             | 9/91 [01:57<19:40, 14.40s/it]\u001b[A\n",
      "Captioning images:  11%|█████████████▏                                                                                                          | 10/91 [02:09<18:21, 13.60s/it]\u001b[A\n",
      "Captioning images:  12%|██████████████▌                                                                                                         | 11/91 [02:23<18:22, 13.78s/it]\u001b[A\n",
      "Captioning images:  13%|███████████████▊                                                                                                        | 12/91 [02:42<19:58, 15.17s/it]\u001b[A\n",
      "Captioning images:  14%|█████████████████▏                                                                                                      | 13/91 [02:56<19:16, 14.83s/it]\u001b[A\n",
      "Captioning images:  15%|██████████████████▍                                                                                                     | 14/91 [03:09<18:12, 14.19s/it]\u001b[A\n",
      "Captioning images:  16%|███████████████████▊                                                                                                    | 15/91 [03:25<18:49, 14.86s/it]\u001b[A\n",
      "Captioning images:  18%|█████████████████████                                                                                                   | 16/91 [03:39<18:21, 14.68s/it]\u001b[A\n",
      "Captioning images:  19%|██████████████████████▍                                                                                                 | 17/91 [03:56<18:57, 15.37s/it]\u001b[A\n",
      "Captioning images:  20%|███████████████████████▋                                                                                                | 18/91 [04:09<17:40, 14.52s/it]\u001b[A\n",
      "Captioning images:  21%|█████████████████████████                                                                                               | 19/91 [04:23<17:16, 14.40s/it]\u001b[A\n",
      "Captioning images:  22%|██████████████████████████▎                                                                                             | 20/91 [04:40<18:10, 15.36s/it]\u001b[A\n",
      "Captioning images:  23%|███████████████████████████▋                                                                                            | 21/91 [04:52<16:33, 14.20s/it]\u001b[A\n",
      "Captioning images:  24%|█████████████████████████████                                                                                           | 22/91 [05:07<16:45, 14.57s/it]\u001b[A\n",
      "Captioning images:  25%|██████████████████████████████▎                                                                                         | 23/91 [05:20<15:48, 13.95s/it]\u001b[A\n",
      "Captioning images:  26%|███████████████████████████████▋                                                                                        | 24/91 [05:31<14:44, 13.20s/it]\u001b[A\n",
      "Captioning images:  27%|████████████████████████████████▉                                                                                       | 25/91 [05:45<14:37, 13.30s/it]\u001b[A\n",
      "Captioning images:  29%|██████████████████████████████████▎                                                                                     | 26/91 [05:57<13:58, 12.90s/it]\u001b[A\n",
      "Captioning images:  30%|███████████████████████████████████▌                                                                                    | 27/91 [06:10<13:48, 12.94s/it]\u001b[A\n",
      "Captioning images:  31%|████████████████████████████████████▉                                                                                   | 28/91 [06:24<13:58, 13.30s/it]\u001b[A\n",
      "Captioning images:  32%|██████████████████████████████████████▏                                                                                 | 29/91 [06:37<13:30, 13.07s/it]\u001b[A\n",
      "Captioning images:  33%|███████████████████████████████████████▌                                                                                | 30/91 [06:50<13:21, 13.14s/it]\u001b[A\n",
      "Captioning images:  34%|████████████████████████████████████████▉                                                                               | 31/91 [07:04<13:35, 13.59s/it]\u001b[A\n",
      "Captioning images:  35%|██████████████████████████████████████████▏                                                                             | 32/91 [07:16<12:48, 13.02s/it]\u001b[A\n",
      "Captioning images:  36%|███████████████████████████████████████████▌                                                                            | 33/91 [07:31<13:11, 13.65s/it]\u001b[A\n",
      "Captioning images:  37%|████████████████████████████████████████████▊                                                                           | 34/91 [07:46<13:20, 14.04s/it]\u001b[A\n",
      "Captioning images:  38%|██████████████████████████████████████████████▏                                                                         | 35/91 [08:18<18:09, 19.45s/it]\u001b[A\n",
      "Captioning images:  40%|███████████████████████████████████████████████▍                                                                        | 36/91 [08:52<21:47, 23.78s/it]\u001b[A\n",
      "Captioning images:  41%|████████████████████████████████████████████████▊                                                                       | 37/91 [09:06<18:46, 20.87s/it]\u001b[A\n",
      "Captioning images:  42%|██████████████████████████████████████████████████                                                                      | 38/91 [09:20<16:30, 18.69s/it]\u001b[A\n",
      "Captioning images:  43%|███████████████████████████████████████████████████▍                                                                    | 39/91 [09:34<14:53, 17.18s/it]\u001b[A\n",
      "Captioning images:  44%|████████████████████████████████████████████████████▋                                                                   | 40/91 [10:08<18:58, 22.33s/it]\u001b[A\n",
      "Captioning images:  45%|██████████████████████████████████████████████████████                                                                  | 41/91 [10:34<19:38, 23.57s/it]\u001b[A\n",
      "Captioning images:  46%|███████████████████████████████████████████████████████▍                                                                | 42/91 [11:08<21:44, 26.62s/it]\u001b[A\n",
      "Captioning images:  47%|████████████████████████████████████████████████████████▋                                                               | 43/91 [11:40<22:36, 28.25s/it]\u001b[A\n",
      "Captioning images:  48%|██████████████████████████████████████████████████████████                                                              | 44/91 [12:14<23:31, 30.03s/it]\u001b[A\n",
      "Captioning images:  49%|███████████████████████████████████████████████████████████▎                                                            | 45/91 [12:44<23:02, 30.06s/it]\u001b[A\n",
      "Captioning images:  51%|████████████████████████████████████████████████████████████▋                                                           | 46/91 [13:14<22:21, 29.81s/it]\u001b[A\n",
      "Captioning images:  52%|█████████████████████████████████████████████████████████████▉                                                          | 47/91 [13:44<21:55, 29.90s/it]\u001b[A\n",
      "Captioning images:  53%|███████████████████████████████████████████████████████████████▎                                                        | 48/91 [14:17<22:12, 31.00s/it]\u001b[A\n",
      "Captioning images:  54%|████████████████████████████████████████████████████████████████▌                                                       | 49/91 [14:48<21:42, 31.01s/it]\u001b[A\n",
      "Captioning images:  55%|█████████████████████████████████████████████████████████████████▉                                                      | 50/91 [15:19<21:06, 30.88s/it]\u001b[A\n",
      "Captioning images:  56%|███████████████████████████████████████████████████████████████████▎                                                    | 51/91 [15:47<20:07, 30.18s/it]\u001b[A\n",
      "Captioning images:  57%|████████████████████████████████████████████████████████████████████▌                                                   | 52/91 [16:18<19:38, 30.21s/it]\u001b[A\n",
      "Captioning images:  58%|█████████████████████████████████████████████████████████████████████▉                                                  | 53/91 [16:48<19:07, 30.19s/it]\u001b[A\n",
      "Captioning images:  59%|███████████████████████████████████████████████████████████████████████▏                                                | 54/91 [17:21<19:08, 31.03s/it]\u001b[A\n",
      "Captioning images:  60%|████████████████████████████████████████████████████████████████████████▌                                               | 55/91 [17:48<17:56, 29.89s/it]\u001b[A\n",
      "Captioning images:  62%|█████████████████████████████████████████████████████████████████████████▊                                              | 56/91 [18:17<17:14, 29.56s/it]\u001b[A\n",
      "Captioning images:  63%|███████████████████████████████████████████████████████████████████████████▏                                            | 57/91 [18:46<16:38, 29.37s/it]\u001b[A\n",
      "Captioning images:  64%|████████████████████████████████████████████████████████████████████████████▍                                           | 58/91 [19:16<16:16, 29.58s/it]\u001b[A\n",
      "Captioning images:  65%|█████████████████████████████████████████████████████████████████████████████▊                                          | 59/91 [19:56<17:25, 32.67s/it]\u001b[A\n",
      "Captioning images:  66%|█████████████████████████████████████████████████████████████████████████████▏                                       | 60/91 [26:54<1:16:41, 148.45s/it]\u001b[A\n",
      "Captioning images:  67%|███████████████████████████████████████████████████████████████████████████████▊                                       | 61/91 [27:27<56:52, 113.75s/it]\u001b[A\n",
      "Captioning images:  68%|█████████████████████████████████████████████████████████████████████████████████▊                                      | 62/91 [28:00<43:18, 89.61s/it]\u001b[A\n",
      "Captioning images:  69%|███████████████████████████████████████████████████████████████████████████████████                                     | 63/91 [28:33<33:52, 72.59s/it]\u001b[A\n",
      "Captioning images:  70%|████████████████████████████████████████████████████████████████████████████████████▍                                   | 64/91 [29:15<28:29, 63.30s/it]\u001b[A\n",
      "Captioning images:  71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 65/91 [29:50<23:43, 54.77s/it]\u001b[A\n",
      "Captioning images:  73%|███████████████████████████████████████████████████████████████████████████████████████                                 | 66/91 [30:24<20:10, 48.43s/it]\u001b[A\n",
      "Captioning images:  74%|████████████████████████████████████████████████████████████████████████████████████████▎                               | 67/91 [30:45<16:09, 40.39s/it]\u001b[A\n",
      "Captioning images:  75%|█████████████████████████████████████████████████████████████████████████████████████████▋                              | 68/91 [30:58<12:16, 32.01s/it]\u001b[A\n",
      "Captioning images:  76%|██████████████████████████████████████████████████████████████████████████████████████████▉                             | 69/91 [31:16<10:15, 27.96s/it]\u001b[A\n",
      "Captioning images:  77%|████████████████████████████████████████████████████████████████████████████████████████████▎                           | 70/91 [31:31<08:24, 24.00s/it]\u001b[A\n",
      "Captioning images:  78%|█████████████████████████████████████████████████████████████████████████████████████████████▋                          | 71/91 [31:43<06:47, 20.37s/it]\u001b[A\n",
      "Captioning images:  79%|██████████████████████████████████████████████████████████████████████████████████████████████▉                         | 72/91 [31:56<05:45, 18.20s/it]\u001b[A\n",
      "Captioning images:  80%|████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 73/91 [32:10<05:04, 16.92s/it]\u001b[A\n",
      "Captioning images:  81%|█████████████████████████████████████████████████████████████████████████████████████████████████▌                      | 74/91 [32:22<04:23, 15.52s/it]\u001b[A\n",
      "Captioning images:  82%|██████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 75/91 [32:36<04:00, 15.03s/it]\u001b[A\n",
      "Captioning images:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                   | 76/91 [32:50<03:40, 14.70s/it]\u001b[A\n",
      "Captioning images:  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 77/91 [33:02<03:15, 13.99s/it]\u001b[A\n",
      "Captioning images:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 78/91 [33:16<03:02, 14.07s/it]\u001b[A\n",
      "Captioning images:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 79/91 [33:30<02:48, 14.04s/it]\u001b[A\n",
      "Captioning images:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 80/91 [33:48<02:46, 15.14s/it]\u001b[A\n",
      "Captioning images:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 81/91 [33:57<02:11, 13.10s/it]\u001b[A\n",
      "Captioning images:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 82/91 [34:06<01:46, 11.88s/it]\u001b[A\n",
      "Captioning images:  91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍          | 83/91 [34:15<01:28, 11.10s/it]\u001b[A\n",
      "Captioning images:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 84/91 [34:27<01:20, 11.56s/it]\u001b[A\n",
      "Captioning images:  93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████        | 85/91 [34:35<01:02, 10.48s/it]\u001b[A\n",
      "Captioning images:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 86/91 [34:44<00:50, 10.06s/it]\u001b[A\n",
      "Captioning images:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 87/91 [34:55<00:41, 10.34s/it]\u001b[A\n",
      "Captioning images:  97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 88/91 [35:06<00:31, 10.34s/it]\u001b[A\n",
      "Captioning images:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 89/91 [35:17<00:20, 10.45s/it]\u001b[A\n",
      "Captioning images:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 90/91 [35:24<00:09,  9.68s/it]\u001b[A\n",
      "Captioning images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [35:28<00:00, 23.39s/it]\u001b[A\n",
      "2024-06-05 21:18:58,279 [INFO] Writing 1449 captions to outputs/eval_captioning/2024_06_05_20_43_29/pred_captions.txt\n",
      "2024-06-05 21:18:58,280 [INFO] Writing 1449 captions to outputs/eval_captioning/2024_06_05_20_43_29/ref_captions.txt\n",
      "2024-06-05 21:18:58,281 [INFO] Evaluating captions\n",
      "2024-06-05 21:18:58,347 [INFO] BLEU 1-grams: 0.387525702535957, 2-grams: 0.2656485182312339, 3-grams: 0.18285709308283307, 4-grams: 0.12737573784685255\n",
      "2024-06-05 21:18:58,348 [INFO] Final BLEU@4: 12.74%\n",
      "2024-06-05 21:18:58,348 [INFO] Scores: {'bleu': 0.12737573784685255}\n",
      "2024-06-05 21:18:58,348 [INFO] Writing scores to outputs/eval_captioning/2024_06_05_20_43_29/scores.json\n"
     ]
    }
   ],
   "source": [
    "# 1.4 Student Hyperparameter Search\n",
    "# TODO: Experiment with different decoding parameters. (Top-K with different K and temperature or greedy decoding). \n",
    "#       Can you improve the BLEU score? Try at least 3 new settings.\n",
    "#       Note the hyperparameters, the prompt and the resulting BLEU score in a table for each setting.\n",
    "#       Add the table to your report. (1 point)\n",
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=100, temperature=0.7, prompt=\"a picture of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf535e-447d-4493-bd4f-e12731007a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3f0cb7-6677-49ed-be36-1891dae6e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Image-Text Retrieval\n",
    "# Your second task will be to train and evaluate the retrieval head of the BLIP model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d87144-6833-4d58-ac2b-9bc8f4e69119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 21:32:45,002 [INFO] Running on device: cpu, cuda available: False\n",
      "2024-06-05 21:32:48,456 [INFO] Done initializing weights for BertModel XBertEncoder.\n",
      "2024-06-05 21:32:48,524 [INFO] Created model BlipRetrieval with 223.7M parameters.\n",
      "2024-06-05 21:32:48,783 [INFO] Reshaped position embedding from 196 to 576\n",
      "2024-06-05 21:32:48,815 [INFO] Missing keys ['vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias']\n",
      "2024-06-05 21:32:48,815 [INFO] Done loading checkpoint from ckpt/blip_model_base.pth\n",
      "2024-06-05 21:32:48,856 [INFO] Done loading retrieval head from ckpt/blip_model_retrieval_head.pth\n",
      "2024-06-05 21:32:48,861 [INFO] Output dir: outputs/eval_retrieval/2024_06_05_21_32_48\n",
      "2024-06-05 21:32:48,861 [INFO] Load dataset from data/VOCdevkit/VOC2012\n",
      "\n",
      "Generating retrieval features for eval:   0%|                                                                                                            | 0/91 [00:00<?, ?it/s]\u001b[A\n",
      "Generating retrieval features for eval:   1%|█                                                                                                   | 1/91 [00:02<04:09,  2.77s/it]\u001b[A\n",
      "Generating retrieval features for eval:   2%|██▏                                                                                                 | 2/91 [00:05<04:07,  2.78s/it]\u001b[A\n",
      "Generating retrieval features for eval:   3%|███▎                                                                                                | 3/91 [00:08<04:03,  2.77s/it]\u001b[A\n",
      "Generating retrieval features for eval:   4%|████▍                                                                                               | 4/91 [00:11<04:03,  2.80s/it]\u001b[A\n",
      "Generating retrieval features for eval:   5%|█████▍                                                                                              | 5/91 [00:14<04:03,  2.83s/it]\u001b[A\n",
      "Generating retrieval features for eval:   7%|██████▌                                                                                             | 6/91 [00:17<04:04,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval:   8%|███████▋                                                                                            | 7/91 [00:19<04:04,  2.91s/it]\u001b[A\n",
      "Generating retrieval features for eval:   9%|████████▊                                                                                           | 8/91 [00:22<04:01,  2.90s/it]\u001b[A\n",
      "Generating retrieval features for eval:  10%|█████████▉                                                                                          | 9/91 [00:25<04:02,  2.96s/it]\u001b[A\n",
      "Generating retrieval features for eval:  11%|██████████▉                                                                                        | 10/91 [00:28<03:56,  2.92s/it]\u001b[A\n",
      "Generating retrieval features for eval:  12%|███████████▉                                                                                       | 11/91 [00:31<03:51,  2.90s/it]\u001b[A\n",
      "Generating retrieval features for eval:  13%|█████████████                                                                                      | 12/91 [00:34<03:52,  2.95s/it]\u001b[A\n",
      "Generating retrieval features for eval:  14%|██████████████▏                                                                                    | 13/91 [00:37<03:47,  2.92s/it]\u001b[A\n",
      "Generating retrieval features for eval:  15%|███████████████▏                                                                                   | 14/91 [00:40<03:41,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval:  16%|████████████████▎                                                                                  | 15/91 [00:43<03:36,  2.84s/it]\u001b[A\n",
      "Generating retrieval features for eval:  18%|█████████████████▍                                                                                 | 16/91 [00:45<03:32,  2.83s/it]\u001b[A\n",
      "Generating retrieval features for eval:  19%|██████████████████▍                                                                                | 17/91 [00:48<03:33,  2.88s/it]\u001b[A\n",
      "Generating retrieval features for eval:  20%|███████████████████▌                                                                               | 18/91 [00:51<03:28,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  21%|████████████████████▋                                                                              | 19/91 [00:54<03:24,  2.83s/it]\u001b[A\n",
      "Generating retrieval features for eval:  22%|█████████████████████▊                                                                             | 20/91 [00:57<03:22,  2.85s/it]\u001b[A\n",
      "Generating retrieval features for eval:  23%|██████████████████████▊                                                                            | 21/91 [01:00<03:23,  2.90s/it]\u001b[A\n",
      "Generating retrieval features for eval:  24%|███████████████████████▉                                                                           | 22/91 [01:03<03:25,  2.97s/it]\u001b[A\n",
      "Generating retrieval features for eval:  25%|█████████████████████████                                                                          | 23/91 [01:06<03:18,  2.91s/it]\u001b[A\n",
      "Generating retrieval features for eval:  26%|██████████████████████████                                                                         | 24/91 [01:09<03:14,  2.90s/it]\u001b[A\n",
      "Generating retrieval features for eval:  27%|███████████████████████████▏                                                                       | 25/91 [01:11<03:08,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  29%|████████████████████████████▎                                                                      | 26/91 [01:14<03:04,  2.85s/it]\u001b[A\n",
      "Generating retrieval features for eval:  30%|█████████████████████████████▎                                                                     | 27/91 [01:17<03:01,  2.83s/it]\u001b[A\n",
      "Generating retrieval features for eval:  31%|██████████████████████████████▍                                                                    | 28/91 [01:20<02:58,  2.83s/it]\u001b[A\n",
      "Generating retrieval features for eval:  32%|███████████████████████████████▌                                                                   | 29/91 [01:23<02:57,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  33%|████████████████████████████████▋                                                                  | 30/91 [01:26<02:54,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  34%|█████████████████████████████████▋                                                                 | 31/91 [01:28<02:50,  2.85s/it]\u001b[A\n",
      "Generating retrieval features for eval:  35%|██████████████████████████████████▊                                                                | 32/91 [01:31<02:48,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  36%|███████████████████████████████████▉                                                               | 33/91 [01:34<02:45,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  37%|████████████████████████████████████▉                                                              | 34/91 [01:37<02:41,  2.83s/it]\u001b[A\n",
      "Generating retrieval features for eval:  38%|██████████████████████████████████████                                                             | 35/91 [01:40<02:38,  2.83s/it]\u001b[A\n",
      "Generating retrieval features for eval:  40%|███████████████████████████████████████▏                                                           | 36/91 [01:43<02:37,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval:  41%|████████████████████████████████████████▎                                                          | 37/91 [01:46<02:35,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval:  42%|█████████████████████████████████████████▎                                                         | 38/91 [01:49<02:32,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval:  43%|██████████████████████████████████████████▍                                                        | 39/91 [01:51<02:28,  2.85s/it]\u001b[A\n",
      "Generating retrieval features for eval:  44%|███████████████████████████████████████████▌                                                       | 40/91 [01:54<02:25,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  45%|████████████████████████████████████████████▌                                                      | 41/91 [01:57<02:22,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  46%|█████████████████████████████████████████████▋                                                     | 42/91 [02:00<02:20,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  47%|██████████████████████████████████████████████▊                                                    | 43/91 [02:03<02:17,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  48%|███████████████████████████████████████████████▊                                                   | 44/91 [02:06<02:13,  2.84s/it]\u001b[A\n",
      "Generating retrieval features for eval:  49%|████████████████████████████████████████████████▉                                                  | 45/91 [02:08<02:10,  2.84s/it]\u001b[A\n",
      "Generating retrieval features for eval:  51%|██████████████████████████████████████████████████                                                 | 46/91 [02:11<02:07,  2.84s/it]\u001b[A\n",
      "Generating retrieval features for eval:  52%|███████████████████████████████████████████████████▏                                               | 47/91 [02:14<02:03,  2.82s/it]\u001b[A\n",
      "Generating retrieval features for eval:  53%|████████████████████████████████████████████████████▏                                              | 48/91 [02:17<02:01,  2.82s/it]\u001b[A\n",
      "Generating retrieval features for eval:  54%|█████████████████████████████████████████████████████▎                                             | 49/91 [02:20<01:58,  2.82s/it]\u001b[A\n",
      "Generating retrieval features for eval:  55%|██████████████████████████████████████████████████████▍                                            | 50/91 [02:22<01:55,  2.81s/it]\u001b[A\n",
      "Generating retrieval features for eval:  56%|███████████████████████████████████████████████████████▍                                           | 51/91 [02:25<01:52,  2.81s/it]\u001b[A\n",
      "Generating retrieval features for eval:  57%|████████████████████████████████████████████████████████▌                                          | 52/91 [02:28<01:50,  2.84s/it]\u001b[A\n",
      "Generating retrieval features for eval:  58%|█████████████████████████████████████████████████████████▋                                         | 53/91 [02:31<01:47,  2.84s/it]\u001b[A\n",
      "Generating retrieval features for eval:  59%|██████████████████████████████████████████████████████████▋                                        | 54/91 [02:34<01:45,  2.85s/it]\u001b[A\n",
      "Generating retrieval features for eval:  60%|███████████████████████████████████████████████████████████▊                                       | 55/91 [02:37<01:43,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  62%|████████████████████████████████████████████████████████████▉                                      | 56/91 [02:40<01:40,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval:  63%|██████████████████████████████████████████████████████████████                                     | 57/91 [02:43<01:37,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval:  64%|███████████████████████████████████████████████████████████████                                    | 58/91 [02:45<01:35,  2.88s/it]\u001b[A\n",
      "Generating retrieval features for eval:  65%|████████████████████████████████████████████████████████████████▏                                  | 59/91 [02:48<01:31,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval:  66%|█████████████████████████████████████████████████████████████████▎                                 | 60/91 [02:51<01:29,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval:  67%|██████████████████████████████████████████████████████████████████▎                                | 61/91 [02:54<01:26,  2.88s/it]\u001b[A\n",
      "Generating retrieval features for eval:  68%|███████████████████████████████████████████████████████████████████▍                               | 62/91 [02:57<01:22,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  69%|████████████████████████████████████████████████████████████████████▌                              | 63/91 [03:00<01:20,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval:  70%|█████████████████████████████████████████████████████████████████████▋                             | 64/91 [03:03<01:17,  2.85s/it]\u001b[A\n",
      "Generating retrieval features for eval:  71%|██████████████████████████████████████████████████████████████████████▋                            | 65/91 [03:05<01:14,  2.85s/it]\u001b[A\n",
      "Generating retrieval features for eval:  73%|███████████████████████████████████████████████████████████████████████▊                           | 66/91 [03:08<01:11,  2.84s/it]\u001b[A\n",
      "Generating retrieval features for eval:  74%|████████████████████████████████████████████████████████████████████████▉                          | 67/91 [03:11<01:08,  2.84s/it]\u001b[A\n",
      "Generating retrieval features for eval:  75%|█████████████████████████████████████████████████████████████████████████▉                         | 68/91 [03:14<01:05,  2.84s/it]\u001b[A\n",
      "Generating retrieval features for eval:  76%|███████████████████████████████████████████████████████████████████████████                        | 69/91 [03:17<01:02,  2.84s/it]\u001b[A\n",
      "Generating retrieval features for eval:  77%|████████████████████████████████████████████████████████████████████████████▏                      | 70/91 [03:20<00:59,  2.83s/it]\u001b[A\n",
      "Generating retrieval features for eval:  78%|█████████████████████████████████████████████████████████████████████████████▏                     | 71/91 [03:22<00:57,  2.85s/it]\u001b[A\n",
      "Generating retrieval features for eval:  79%|██████████████████████████████████████████████████████████████████████████████▎                    | 72/91 [03:25<00:54,  2.85s/it]\u001b[A\n",
      "Generating retrieval features for eval:  80%|███████████████████████████████████████████████████████████████████████████████▍                   | 73/91 [03:28<00:51,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval:  81%|████████████████████████████████████████████████████████████████████████████████▌                  | 74/91 [03:32<00:51,  3.05s/it]\u001b[A\n",
      "Generating retrieval features for eval:  82%|█████████████████████████████████████████████████████████████████████████████████▌                 | 75/91 [03:35<00:47,  3.00s/it]\u001b[A\n",
      "Generating retrieval features for eval:  84%|██████████████████████████████████████████████████████████████████████████████████▋                | 76/91 [03:37<00:44,  2.96s/it]\u001b[A\n",
      "Generating retrieval features for eval:  85%|███████████████████████████████████████████████████████████████████████████████████▊               | 77/91 [03:40<00:41,  2.94s/it]\u001b[A\n",
      "Generating retrieval features for eval:  86%|████████████████████████████████████████████████████████████████████████████████████▊              | 78/91 [03:43<00:38,  2.96s/it]\u001b[A\n",
      "Generating retrieval features for eval:  87%|█████████████████████████████████████████████████████████████████████████████████████▉             | 79/91 [03:46<00:35,  2.95s/it]\u001b[A\n",
      "Generating retrieval features for eval:  88%|███████████████████████████████████████████████████████████████████████████████████████            | 80/91 [03:49<00:32,  2.92s/it]\u001b[A\n",
      "Generating retrieval features for eval:  89%|████████████████████████████████████████████████████████████████████████████████████████           | 81/91 [03:52<00:28,  2.88s/it]\u001b[A\n",
      "Generating retrieval features for eval:  90%|█████████████████████████████████████████████████████████████████████████████████████████▏         | 82/91 [03:55<00:25,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  91%|██████████████████████████████████████████████████████████████████████████████████████████▎        | 83/91 [03:58<00:22,  2.86s/it]\u001b[A\n",
      "Generating retrieval features for eval:  92%|███████████████████████████████████████████████████████████████████████████████████████████▍       | 84/91 [04:00<00:19,  2.84s/it]\u001b[A\n",
      "Generating retrieval features for eval:  93%|████████████████████████████████████████████████████████████████████████████████████████████▍      | 85/91 [04:03<00:17,  2.85s/it]\u001b[A\n",
      "Generating retrieval features for eval:  95%|█████████████████████████████████████████████████████████████████████████████████████████████▌     | 86/91 [04:06<00:14,  2.83s/it]\u001b[A\n",
      "Generating retrieval features for eval:  96%|██████████████████████████████████████████████████████████████████████████████████████████████▋    | 87/91 [04:09<00:11,  2.82s/it]\u001b[A\n",
      "Generating retrieval features for eval:  97%|███████████████████████████████████████████████████████████████████████████████████████████████▋   | 88/91 [04:12<00:08,  2.82s/it]\u001b[A\n",
      "Generating retrieval features for eval:  98%|████████████████████████████████████████████████████████████████████████████████████████████████▊  | 89/91 [04:14<00:05,  2.82s/it]\u001b[A\n",
      "Generating retrieval features for eval:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████▉ | 90/91 [04:17<00:02,  2.87s/it]\u001b[A\n",
      "Generating retrieval features for eval: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:19<00:00,  2.85s/it]\u001b[A\n",
      "2024-06-05 21:37:08,720 [INFO] Validation results: {'i_r1': 0.533471359558316, 'i_r5': 0.8067632850241546, 'i_r10': 0.906832298136646, 'i_medr': 1.0, 'i_meanr': 4.0690131124913735, 't_r1': 0.5355417529330573, 't_r5': 0.8184955141476881, 't_r10': 0.8923395445134575, 't_medr': 1.0, 't_meanr': 4.680469289164941}\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Complete Forward Pass and Evaluate\n",
    "#     Todo: Complete the forward pass in file models/blip/blip_retrieval.py. \n",
    "#           Evaluate your implementation with the provided checkpoint. \n",
    "#           You should get about 54% image-to-text R@1. (2 points)\n",
    "from eval_retrieval import eval_without_args    \n",
    "eval_without_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d4778a-aa46-4933-95c0-98b66b697bdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 22:55:42,286 [INFO] Running on device: cpu, cuda available: False\n",
      "Captioning images:  11%|████████████▌                                                                                                     | 10/91 [2:45:40<22:21:57, 994.05s/it]\n",
      "2024-06-05 22:55:45,835 [INFO] Done initializing weights for BertModel XBertEncoder.\n",
      "2024-06-05 22:55:45,892 [INFO] Created model BlipRetrieval with 223.7M parameters.\n",
      "2024-06-05 22:55:46,186 [INFO] Reshaped position embedding from 196 to 576\n",
      "2024-06-05 22:55:46,225 [INFO] Missing keys ['vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias']\n",
      "2024-06-05 22:55:46,225 [INFO] Done loading checkpoint from ckpt/blip_model_base.pth\n",
      "2024-06-05 22:55:46,256 [INFO] Will train vision_proj.weight with shape torch.Size([256, 768])\n",
      "2024-06-05 22:55:46,256 [INFO] Will train vision_proj.bias with shape torch.Size([256])\n",
      "2024-06-05 22:55:46,256 [INFO] Will train text_proj.weight with shape torch.Size([256, 768])\n",
      "2024-06-05 22:55:46,257 [INFO] Will train text_proj.bias with shape torch.Size([256])\n",
      "2024-06-05 22:55:46,257 [INFO] Load dataset from data/VOCdevkit/VOC2012\n",
      "2024-06-05 22:55:46,324 [INFO] Output dir: outputs/train_retrieval/2024_06_05_22_55_46\n",
      "2024-06-05 22:55:46,326 [INFO] Training epoch 0\n",
      "2024-06-05 22:55:49,565 [INFO]   step: 0 loss: 2.779 lr: 1.000e-03                                                                                                              \n",
      "Training:   1%|█▍                                                                                                                                | 1/91 [00:03<04:51,  3.24s/it]/Users/e.lushtaku/E/Personal/MasterStudies-UniversityOfFreiburg/SecondSemester/DeepLearningLab/Exercise3/DL_LAB24_Exercises-3/models/preprocessing/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative\n",
      "  offset = -low * scale\n",
      "2024-06-05 22:56:19,214 [INFO]   step: 10 loss: 1.292 lr: 9.780e-04                                                                                                             \n",
      "2024-06-05 22:56:48,339 [INFO]   step: 20 loss: 0.417 lr: 9.560e-04                                                                                                             \n",
      "2024-06-05 22:57:17,682 [INFO]   step: 30 loss: 0.669 lr: 9.341e-04                                                                                                             \n",
      "2024-06-05 22:57:47,273 [INFO]   step: 40 loss: 0.510 lr: 9.121e-04                                                                                                             \n",
      "2024-06-05 22:58:16,573 [INFO]   step: 50 loss: 0.244 lr: 8.901e-04                                                                                                             \n",
      "2024-06-05 22:58:46,099 [INFO]   step: 60 loss: 0.426 lr: 8.681e-04                                                                                                             \n",
      "2024-06-05 22:59:15,119 [INFO]   step: 70 loss: 0.601 lr: 8.462e-04                                                                                                             \n",
      "2024-06-05 22:59:44,299 [INFO]   step: 80 loss: 0.214 lr: 8.242e-04                                                                                                             \n",
      "2024-06-05 23:00:13,602 [INFO]   step: 90 loss: 0.251 lr: 8.022e-04                                                                                                             \n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:27<00:00,  2.94s/it]\n",
      "Generating retrieval features for eval: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:26<00:00,  2.92s/it]\n",
      "2024-06-05 23:04:39,895 [INFO] Validation results: {'i_r1': 0.3326432022084196, 'i_r5': 0.6528640441683919, 'i_r10': 0.7791580400276052, 'i_medr': 3.0, 'i_meanr': 8.936507936507937, 't_r1': 0.34299516908212563, 't_r5': 0.6521739130434783, 't_r10': 0.7763975155279503, 't_medr': 3.0, 't_meanr': 9.052449965493444}\n",
      "2024-06-05 23:04:39,902 [INFO] Training epoch 1\n",
      "2024-06-05 23:04:42,947 [INFO]   step: 0 loss: 0.153 lr: 8.000e-04                                                                                                              \n",
      "2024-06-05 23:05:12,652 [INFO]   step: 10 loss: 0.201 lr: 7.780e-04                                                                                                             \n",
      "2024-06-05 23:05:42,036 [INFO]   step: 20 loss: 0.202 lr: 7.560e-04                                                                                                             \n",
      "2024-06-05 23:06:11,931 [INFO]   step: 30 loss: 0.325 lr: 7.341e-04                                                                                                             \n",
      "2024-06-05 23:06:42,138 [INFO]   step: 40 loss: 0.168 lr: 7.121e-04                                                                                                             \n",
      "2024-06-05 23:07:12,101 [INFO]   step: 50 loss: 0.302 lr: 6.901e-04                                                                                                             \n",
      "2024-06-05 23:07:41,596 [INFO]   step: 60 loss: 0.401 lr: 6.681e-04                                                                                                             \n",
      "2024-06-05 23:08:10,970 [INFO]   step: 70 loss: 0.202 lr: 6.462e-04                                                                                                             \n",
      "2024-06-05 23:08:40,432 [INFO]   step: 80 loss: 0.108 lr: 6.242e-04                                                                                                             \n",
      "2024-06-05 23:09:09,664 [INFO]   step: 90 loss: 0.165 lr: 6.022e-04                                                                                                             \n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:29<00:00,  2.96s/it]\n",
      "Generating retrieval features for eval: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:27<00:00,  2.94s/it]\n",
      "2024-06-05 23:13:37,367 [INFO] Validation results: {'i_r1': 0.37612146307798483, 'i_r5': 0.7032436162870945, 'i_r10': 0.8295376121463078, 'i_medr': 2.0, 'i_meanr': 7.2146307798481715, 't_r1': 0.36438923395445133, 't_r5': 0.6749482401656315, 't_r10': 0.7998619737750172, 't_medr': 2.0, 't_meanr': 7.663906142167011}\n",
      "2024-06-05 23:13:37,376 [INFO] Training epoch 2\n",
      "2024-06-05 23:13:40,398 [INFO]   step: 0 loss: 0.395 lr: 6.000e-04                                                                                                              \n",
      "2024-06-05 23:14:10,934 [INFO]   step: 10 loss: 0.053 lr: 5.780e-04                                                                                                             \n",
      "2024-06-05 23:14:41,227 [INFO]   step: 20 loss: 0.101 lr: 5.560e-04                                                                                                             \n",
      "2024-06-05 23:15:11,320 [INFO]   step: 30 loss: 0.185 lr: 5.341e-04                                                                                                             \n",
      "2024-06-05 23:15:41,903 [INFO]   step: 40 loss: 0.102 lr: 5.121e-04                                                                                                             \n",
      "2024-06-05 23:16:12,742 [INFO]   step: 50 loss: 0.341 lr: 4.901e-04                                                                                                             \n",
      "2024-06-05 23:16:43,004 [INFO]   step: 60 loss: 0.103 lr: 4.681e-04                                                                                                             \n",
      "2024-06-05 23:17:13,352 [INFO]   step: 70 loss: 0.213 lr: 4.462e-04                                                                                                             \n",
      "2024-06-05 23:17:43,508 [INFO]   step: 80 loss: 0.158 lr: 4.242e-04                                                                                                             \n",
      "2024-06-05 23:18:13,567 [INFO]   step: 90 loss: 0.121 lr: 4.022e-04                                                                                                             \n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:36<00:00,  3.04s/it]\n",
      "Generating retrieval features for eval: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:27<00:00,  2.94s/it]\n",
      "2024-06-05 23:22:41,455 [INFO] Validation results: {'i_r1': 0.4140786749482402, 'i_r5': 0.7322291235334714, 'i_r10': 0.8571428571428571, 'i_medr': 2.0, 'i_meanr': 6.495514147688061, 't_r1': 0.3947550034506556, 't_r5': 0.704623878536922, 't_r10': 0.8343685300207039, 't_medr': 2.0, 't_meanr': 6.849551414768806}\n",
      "2024-06-05 23:22:41,461 [INFO] Training epoch 3\n",
      "2024-06-05 23:22:44,448 [INFO]   step: 0 loss: 0.117 lr: 4.000e-04                                                                                                              \n",
      "2024-06-05 23:23:14,479 [INFO]   step: 10 loss: 0.147 lr: 3.780e-04                                                                                                             \n",
      "2024-06-05 23:23:44,871 [INFO]   step: 20 loss: 0.103 lr: 3.560e-04                                                                                                             \n",
      "2024-06-05 23:24:15,516 [INFO]   step: 30 loss: 0.448 lr: 3.341e-04                                                                                                             \n",
      "2024-06-05 23:24:45,672 [INFO]   step: 40 loss: 0.115 lr: 3.121e-04                                                                                                             \n",
      "2024-06-05 23:25:15,783 [INFO]   step: 50 loss: 0.083 lr: 2.901e-04                                                                                                             \n",
      "2024-06-05 23:25:45,838 [INFO]   step: 60 loss: 0.178 lr: 2.681e-04                                                                                                             \n",
      "2024-06-05 23:26:16,308 [INFO]   step: 70 loss: 0.104 lr: 2.462e-04                                                                                                             \n",
      "2024-06-05 23:26:46,182 [INFO]   step: 80 loss: 0.039 lr: 2.242e-04                                                                                                             \n",
      "2024-06-05 23:27:16,293 [INFO]   step: 90 loss: 0.180 lr: 2.022e-04                                                                                                             \n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:34<00:00,  3.02s/it]\n",
      "Generating retrieval features for eval: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:24<00:00,  2.91s/it]\n",
      "2024-06-05 23:31:41,065 [INFO] Validation results: {'i_r1': 0.432712215320911, 'i_r5': 0.7501725327812284, 'i_r10': 0.873015873015873, 'i_medr': 2.0, 'i_meanr': 5.901311249137336, 't_r1': 0.4161490683229814, 't_r5': 0.7329192546583851, 't_r10': 0.8378191856452726, 't_medr': 2.0, 't_meanr': 6.527950310559007}\n",
      "2024-06-05 23:31:41,074 [INFO] Training epoch 4\n",
      "2024-06-05 23:31:44,171 [INFO]   step: 0 loss: 0.109 lr: 2.000e-04                                                                                                              \n",
      "2024-06-05 23:32:14,772 [INFO]   step: 10 loss: 0.136 lr: 1.780e-04                                                                                                             \n",
      "2024-06-05 23:32:44,212 [INFO]   step: 20 loss: 0.136 lr: 1.560e-04                                                                                                             \n",
      "2024-06-05 23:33:13,416 [INFO]   step: 30 loss: 0.116 lr: 1.341e-04                                                                                                             \n",
      "2024-06-05 23:33:42,962 [INFO]   step: 40 loss: 0.060 lr: 1.121e-04                                                                                                             \n",
      "2024-06-05 23:34:12,557 [INFO]   step: 50 loss: 0.076 lr: 9.011e-05                                                                                                             \n",
      "2024-06-05 23:34:42,136 [INFO]   step: 60 loss: 0.136 lr: 6.813e-05                                                                                                             \n",
      "2024-06-05 23:35:11,524 [INFO]   step: 70 loss: 0.123 lr: 4.615e-05                                                                                                             \n",
      "2024-06-05 23:35:41,116 [INFO]   step: 80 loss: 0.074 lr: 2.418e-05                                                                                                             \n",
      "2024-06-05 23:36:10,512 [INFO]   step: 90 loss: 0.068 lr: 2.198e-06                                                                                                             \n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:29<00:00,  2.96s/it]\n",
      "Generating retrieval features for eval: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:21<00:00,  2.87s/it]\n",
      "2024-06-05 23:40:31,761 [INFO] Validation results: {'i_r1': 0.4492753623188406, 'i_r5': 0.7556935817805382, 'i_r10': 0.8702553485162181, 'i_medr': 2.0, 'i_meanr': 5.715665976535542, 't_r1': 0.42581090407177363, 't_r5': 0.7342995169082126, 't_r10': 0.8454106280193237, 't_medr': 2.0, 't_meanr': 6.358178053830228}\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Complete Loss and Train from Scratch\n",
    "# Todo: Complete the loss computation in file train_retrieval.py function train_epoch. \n",
    "#       Train the retrieval projection layers from scratch (i.e. from random initialization).\n",
    "#       You should get about 43% image-to-text R@1. (1 point)\n",
    "from train_retrieval import train_retrieval_without_args\n",
    "train_retrieval_without_args(finetune=False, learning_rate=1e-3, weight_decay=1e-3, epochs=5, temperature=0.1)\n",
    "\n",
    "# Optional: start a tensorboard server tensorboard --logdir outputs --port 6006 and watch the experiment in the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb532e-65e3-4e24-8fa9-2a26e3ea6a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 23:46:13,923 [INFO] Running on device: cpu, cuda available: False\n",
      "2024-06-05 23:46:17,292 [INFO] Done initializing weights for BertModel XBertEncoder.\n",
      "2024-06-05 23:46:17,345 [INFO] Created model BlipRetrieval with 223.7M parameters.\n",
      "2024-06-05 23:46:17,588 [INFO] Reshaped position embedding from 196 to 576\n",
      "2024-06-05 23:46:17,619 [INFO] Missing keys ['vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias']\n",
      "2024-06-05 23:46:17,620 [INFO] Done loading checkpoint from ckpt/blip_model_base.pth\n",
      "2024-06-05 23:46:17,665 [INFO] Done loading retrieval head from ckpt/blip_model_retrieval_head.pth\n",
      "2024-06-05 23:46:17,669 [INFO] Will train vision_proj.weight with shape torch.Size([256, 768])\n",
      "2024-06-05 23:46:17,669 [INFO] Will train vision_proj.bias with shape torch.Size([256])\n",
      "2024-06-05 23:46:17,670 [INFO] Will train text_proj.weight with shape torch.Size([256, 768])\n",
      "2024-06-05 23:46:17,670 [INFO] Will train text_proj.bias with shape torch.Size([256])\n",
      "2024-06-05 23:46:17,670 [INFO] Load dataset from data/VOCdevkit/VOC2012\n",
      "2024-06-05 23:46:17,732 [INFO] Output dir: outputs/train_retrieval/2024_06_05_23_46_17\n",
      "2024-06-05 23:46:17,733 [INFO] Training epoch 0\n",
      "2024-06-05 23:46:20,683 [INFO]   step: 0 loss: 1.558 lr: 1.000e-05                                                                                                              \n",
      "2024-06-05 23:46:51,257 [INFO]   step: 10 loss: 1.592 lr: 9.634e-06                                                                                                             \n",
      "2024-06-05 23:47:20,885 [INFO]   step: 20 loss: 1.524 lr: 9.267e-06                                                                                                             \n",
      "2024-06-05 23:47:50,126 [INFO]   step: 30 loss: 1.637 lr: 8.901e-06                                                                                                             \n",
      "2024-06-05 23:48:19,362 [INFO]   step: 40 loss: 1.566 lr: 8.535e-06                                                                                                             \n",
      "2024-06-05 23:48:49,358 [INFO]   step: 50 loss: 1.445 lr: 8.168e-06                                                                                                             \n",
      "2024-06-05 23:49:19,089 [INFO]   step: 60 loss: 1.567 lr: 7.802e-06                                                                                                             \n",
      "2024-06-05 23:49:48,542 [INFO]   step: 70 loss: 1.350 lr: 7.436e-06                                                                                                             \n",
      "2024-06-05 23:50:17,694 [INFO]   step: 80 loss: 1.409 lr: 7.070e-06                                                                                                             \n",
      "2024-06-05 23:50:46,751 [INFO]   step: 90 loss: 1.228 lr: 6.703e-06                                                                                                             \n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:29<00:00,  2.96s/it]\n",
      "Generating retrieval features for eval:  58%|█████████████████████████████████████████████████████████▋                                         | 53/91 [02:31<01:48,  2.86s/it]"
     ]
    }
   ],
   "source": [
    "# 2.3 Finetune instead of Train from Scratch\n",
    "# Todo: Now, try finetuning the head instead with --finetune. \n",
    "#       Set learning rate to 1e-5, weight decay to 0 and train for 3 epochs. \n",
    "#       What score do you get and how can you explain the difference to the score when training from scratch? (1 point)\n",
    "#       Try different search queries. What do you observe?\n",
    "    \n",
    "train_retrieval_without_args(finetune=True, learning_rate=1e-5, weight_decay=1e-3, epochs=3, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64660e3d-13a2-4fd9-bea9-bd22f32c3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Student Hyperparameter Search\n",
    "# Todo: Experiment with different hyperparameters in the random initialization setting (i.e. without finetuning). \n",
    "#       Try at least 3 new hyperparameter settings.\n",
    "#       Note the hyperparameters and the resulting image-to-text R@1 score in a table for each setting. \n",
    "#       Can you improve over the baselines? Add the table to your report. (1 point)\n",
    "\n",
    "train_retrieval_without_args(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de0659-9b4b-4875-815a-b58a82cfc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize Top 10 results for a search query.\n",
    "\n",
    "from search_retrieval import get_top10\n",
    "\n",
    "search_query = \"a picture of a plane\"\n",
    "dict_top10 = get_top10(eval_ckpt=None, query = search_query)\n",
    "\n",
    "from PIL import Image\n",
    "for i in range(len(dict_top10[\"id\"])):\n",
    "    image_pil = Image.open(dict_top10[\"fname\"][i])\n",
    "    display(image_pil)\n",
    "    print(f\"Sim. Score: {dict_top10['sim'][i]}\")\n",
    "    print(f\"Caption: {dict_top10['caption'][i]}\")\n",
    "    #print(f\"Name: {dict_top10['name'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb8b9f-1e23-46cb-8784-13d8d8065b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
